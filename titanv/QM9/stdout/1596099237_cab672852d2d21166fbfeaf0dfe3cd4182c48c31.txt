{'dataset_name': 'QM9', 'target': 6, 'model': 'NLG_R', 'seed': 777, 'hyperparams': {'batch_size': 64, 'epochs': 601, 'learning_rate': 0.0001, 'step_size': 30, 'decay_rate': 0.85}, 'architecture': {'layers': 4, 'hidden': 512, 'pooling': 'add', 'JK': 'cat', 'nonlinear_conv': 'CB', 'variants': {'BN': 'N', 'aggr_mlp': 1, 'fea_activation': 'ReLU'}}, 'commit_id': 'cab672852d2d21166fbfeaf0dfe3cd4182c48c31', 'time_stamp': '1596099237', 'directory': '../../nlg_results/titanv/QM9/board/'}
--------
QM9_6, 1596099237_cab6728_CB_1_ReLU_4_512_N_0.0001_30_0.85_B64, ID=cab672852d2d21166fbfeaf0dfe3cd4182c48c31
Epoch: 001, LR: 0.000100, Loss: 0.0438685, Validation MAE: 0.0032086, Test MAE: 0.0032214
Epoch: 002, LR: 0.000100, Loss: 0.0076711, Validation MAE: 0.0011854, Test MAE: 0.0012277
Epoch: 003, LR: 0.000100, Loss: 0.0052160, Validation MAE: 0.0011009, Test MAE: 0.0010881
Epoch: 004, LR: 0.000100, Loss: 0.0043825, Validation MAE: 0.0010079, Test MAE: 0.0010023
Epoch: 005, LR: 0.000100, Loss: 0.0035007, Validation MAE: 0.0009133, Test MAE: 0.0009052
Epoch: 006, LR: 0.000100, Loss: 0.0031452, Validation MAE: 0.0008836, Test MAE: 0.0008756
Epoch: 007, 0.000100,0.0027120,0.0010925,0.0010998
Epoch: 008, 0.000100,0.0025436,0.0010801,0.0010693
Epoch: 009, 0.000100,0.0022791,0.0011263,0.0011250
Epoch: 010, 0.000100,0.0020780,0.0009736,0.0009710
Epoch: 011, 0.000100,0.0018097,0.0009079,0.0009100
Epoch: 012, 0.000100,0.0017256,0.0014414,0.0014244
Epoch: 013, LR: 0.000100, Loss: 0.0015925, Validation MAE: 0.0007405, Test MAE: 0.0007374
Epoch: 014, 0.000100,0.0016104,0.0009837,0.0009770
Epoch: 015, LR: 0.000100, Loss: 0.0013314, Validation MAE: 0.0006579, Test MAE: 0.0006579
Epoch: 016, 0.000100,0.0015248,0.0011852,0.0011755
Epoch: 017, LR: 0.000100, Loss: 0.0010906, Validation MAE: 0.0005995, Test MAE: 0.0005983
Epoch: 018, 0.000100,0.0011250,0.0008787,0.0008795
Epoch: 019, LR: 0.000100, Loss: 0.0009657, Validation MAE: 0.0005620, Test MAE: 0.0005616
Epoch: 020, 0.000100,0.0011001,0.0007999,0.0007953
Epoch: 021, 0.000100,0.0009884,0.0012217,0.0012217
Epoch: 022, 0.000100,0.0010534,0.0006233,0.0006249
Epoch: 023, LR: 0.000100, Loss: 0.0008043, Validation MAE: 0.0005369, Test MAE: 0.0005383
Epoch: 024, 0.000100,0.0007814,0.0007369,0.0007416
Epoch: 025, 0.000100,0.0006969,0.0009344,0.0009335
Epoch: 026, LR: 0.000100, Loss: 0.0007597, Validation MAE: 0.0004660, Test MAE: 0.0004655
Epoch: 027, 0.000100,0.0006327,0.0007997,0.0007970
Epoch: 028, LR: 0.000100, Loss: 0.0006481, Validation MAE: 0.0004645, Test MAE: 0.0004641
Epoch: 029, 0.000100,0.0005982,0.0004706,0.0004778
Epoch: 030, 0.000100,0.0006083,0.0006866,0.0006849
Epoch: 031, 0.000085,0.0004455,0.0005844,0.0005904
Epoch: 032, 0.000085,0.0004858,0.0005043,0.0005140
Epoch: 033, 0.000085,0.0004234,0.0004812,0.0004822
Epoch: 034, 0.000085,0.0004648,0.0004956,0.0005006
Epoch: 035, 0.000085,0.0003941,0.0009134,0.0009149
Epoch: 036, 0.000085,0.0003726,0.0004727,0.0004755
Epoch: 037, 0.000085,0.0003661,0.0005277,0.0005297
Epoch: 038, 0.000085,0.0003562,0.0004960,0.0004993
Epoch: 039, LR: 0.000085, Loss: 0.0003382, Validation MAE: 0.0003834, Test MAE: 0.0003853
Epoch: 040, LR: 0.000085, Loss: 0.0003321, Validation MAE: 0.0003761, Test MAE: 0.0003757
Epoch: 041, 0.000085,0.0003250,0.0006965,0.0007050
Epoch: 042, LR: 0.000085, Loss: 0.0003407, Validation MAE: 0.0003689, Test MAE: 0.0003733
Epoch: 043, 0.000085,0.0003192,0.0004756,0.0004790
Epoch: 044, LR: 0.000085, Loss: 0.0002919, Validation MAE: 0.0003469, Test MAE: 0.0003479
Epoch: 045, 0.000085,0.0002860,0.0004565,0.0004636
Epoch: 046, 0.000085,0.0002743,0.0003851,0.0003887
Epoch: 047, 0.000085,0.0003360,0.0004223,0.0004234
Epoch: 048, LR: 0.000085, Loss: 0.0002531, Validation MAE: 0.0003270, Test MAE: 0.0003294
Epoch: 049, 0.000085,0.0002659,0.0008086,0.0008086
Epoch: 050, 0.000085,0.0002960,0.0004873,0.0004897
Epoch: 051, 0.000085,0.0002436,0.0003862,0.0003919
Epoch: 052, 0.000085,0.0002532,0.0003368,0.0003431
Epoch: 053, 0.000085,0.0002286,0.0004316,0.0004381
Epoch: 054, 0.000085,0.0002412,0.0003395,0.0003437
Epoch: 055, LR: 0.000085, Loss: 0.0002370, Validation MAE: 0.0003099, Test MAE: 0.0003127
Epoch: 056, 0.000085,0.0002349,0.0003675,0.0003686
Epoch: 057, LR: 0.000085, Loss: 0.0002159, Validation MAE: 0.0002834, Test MAE: 0.0002857
Epoch: 058, 0.000085,0.0002082,0.0008028,0.0008080
Epoch: 059, 0.000085,0.0002292,0.0004956,0.0004976
Epoch: 060, LR: 0.000085, Loss: 0.0002092, Validation MAE: 0.0002692, Test MAE: 0.0002760
Epoch: 061, 0.000072,0.0001666,0.0002943,0.0002981
Epoch: 062, 0.000072,0.0001703,0.0003116,0.0003142
Epoch: 063, 0.000072,0.0001672,0.0003115,0.0003151
Epoch: 064, 0.000072,0.0001662,0.0003690,0.0003734
Epoch: 065, 0.000072,0.0001803,0.0003322,0.0003383
Epoch: 066, 0.000072,0.0001729,0.0003095,0.0003148
Epoch: 067, 0.000072,0.0001417,0.0002810,0.0002820
Epoch: 068, LR: 0.000072, Loss: 0.0001568, Validation MAE: 0.0002683, Test MAE: 0.0002690
Epoch: 069, 0.000072,0.0001689,0.0004236,0.0004297
Epoch: 070, LR: 0.000072, Loss: 0.0001484, Validation MAE: 0.0002667, Test MAE: 0.0002698
Epoch: 071, 0.000072,0.0001434,0.0003887,0.0003876
Epoch: 072, 0.000072,0.0001437,0.0003318,0.0003297
Epoch: 073, 0.000072,0.0001473,0.0004062,0.0004052
Epoch: 074, 0.000072,0.0001464,0.0002943,0.0002958
Epoch: 075, 0.000072,0.0001388,0.0002836,0.0002847
Epoch: 076, LR: 0.000072, Loss: 0.0001362, Validation MAE: 0.0002554, Test MAE: 0.0002559
Epoch: 077, 0.000072,0.0001431,0.0002663,0.0002692
Epoch: 078, 0.000072,0.0001463,0.0007243,0.0007214
Epoch: 079, 0.000072,0.0001348,0.0002999,0.0003013
Epoch: 080, 0.000072,0.0001285,0.0003077,0.0003076
Epoch: 081, 0.000072,0.0001219,0.0002671,0.0002699
Epoch: 082, 0.000072,0.0001354,0.0003653,0.0003752
Epoch: 083, 0.000072,0.0001191,0.0002591,0.0002644
Epoch: 084, LR: 0.000072, Loss: 0.0001257, Validation MAE: 0.0002447, Test MAE: 0.0002459
Epoch: 085, 0.000072,0.0001259,0.0005810,0.0005916
Epoch: 086, 0.000072,0.0001183,0.0003606,0.0003568
Epoch: 087, 0.000072,0.0001231,0.0002572,0.0002600
Epoch: 088, 0.000072,0.0001155,0.0003700,0.0003658
Epoch: 089, 0.000072,0.0001191,0.0003021,0.0003041
Epoch: 090, 0.000072,0.0001217,0.0003686,0.0003695
Epoch: 091, 0.000061,0.0001031,0.0002547,0.0002565
Epoch: 092, 0.000061,0.0000890,0.0004907,0.0004971
Epoch: 093, 0.000061,0.0000908,0.0002637,0.0002620
Epoch: 094, 0.000061,0.0000930,0.0004346,0.0004347
Epoch: 095, 0.000061,0.0000957,0.0003157,0.0003217
Epoch: 096, 0.000061,0.0000910,0.0003853,0.0003913
Epoch: 097, 0.000061,0.0000918,0.0002656,0.0002693
Epoch: 098, LR: 0.000061, Loss: 0.0000918, Validation MAE: 0.0002221, Test MAE: 0.0002251
Epoch: 099, 0.000061,0.0000827,0.0002442,0.0002457
Epoch: 100, 0.000061,0.0000939,0.0002311,0.0002329
Epoch: 101, 0.000061,0.0000782,0.0002283,0.0002295
Epoch: 102, 0.000061,0.0000832,0.0002512,0.0002552
Epoch: 103, 0.000061,0.0000832,0.0002267,0.0002260
Epoch: 104, 0.000061,0.0000832,0.0003021,0.0003017
Epoch: 105, 0.000061,0.0000850,0.0002377,0.0002374
Epoch: 106, 0.000061,0.0000821,0.0002226,0.0002276
Epoch: 107, 0.000061,0.0000770,0.0004371,0.0004381
Epoch: 108, 0.000061,0.0000762,0.0002259,0.0002278
Epoch: 109, 0.000061,0.0000860,0.0004247,0.0004226
Epoch: 110, 0.000061,0.0000784,0.0002512,0.0002518
Epoch: 111, LR: 0.000061, Loss: 0.0000795, Validation MAE: 0.0002121, Test MAE: 0.0002139
Epoch: 112, 0.000061,0.0000782,0.0002606,0.0002633
Epoch: 113, 0.000061,0.0000787,0.0002695,0.0002738
Epoch: 114, 0.000061,0.0000777,0.0002616,0.0002597
Epoch: 115, 0.000061,0.0000741,0.0002227,0.0002258
Epoch: 116, 0.000061,0.0000772,0.0002130,0.0002167
Epoch: 117, 0.000061,0.0000724,0.0003226,0.0003172
Epoch: 118, 0.000061,0.0000775,0.0002136,0.0002153
Epoch: 119, 0.000061,0.0000740,0.0002741,0.0002788
Epoch: 120, 0.000061,0.0000688,0.0002591,0.0002580
Epoch: 121, 0.000052,0.0000577,0.0002159,0.0002215
Epoch: 122, 0.000052,0.0000592,0.0002156,0.0002156
Epoch: 123, 0.000052,0.0000609,0.0002669,0.0002714
Epoch: 124, 0.000052,0.0000577,0.0002240,0.0002239
Epoch: 125, 0.000052,0.0000571,0.0002581,0.0002600
Epoch: 126, LR: 0.000052, Loss: 0.0000542, Validation MAE: 0.0002114, Test MAE: 0.0002143
Epoch: 127, 0.000052,0.0000597,0.0002137,0.0002127
Epoch: 128, 0.000052,0.0000558,0.0002625,0.0002688
Epoch: 129, 0.000052,0.0000542,0.0002493,0.0002523
Epoch: 130, LR: 0.000052, Loss: 0.0000581, Validation MAE: 0.0002100, Test MAE: 0.0002120
Epoch: 131, 0.000052,0.0000537,0.0002510,0.0002530
Epoch: 132, 0.000052,0.0000536,0.0003516,0.0003586
Traceback (most recent call last):
  File "./main.py", line 108, in <module>
    loss = train()
  File "./main.py", line 32, in train
    loss.backward()
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/torch/tensor.py", line 107, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.78 GiB total capacity; 351.33 MiB already allocated; 12.44 MiB free; 64.67 MiB cached)
