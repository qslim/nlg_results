{'dataset_name': 'ogbg-code', 'seed': 777, 'num_workers': 0, 'feature': 'full', 'max_seq_len': 5, 'num_vocab': 5000, 'hyperparams': {'batch_size': 128, 'epochs': 51, 'learning_rate': 0.0002, 'step_size': 20, 'decay_rate': 0.85}, 'architecture': {'layers': 4, 'hidden': 512, 'pooling': 'mean', 'JK': 'cat', 'nonlinear_conv': 'EB5', 'dropout': 0.4, 'variants': {'BN': 'Y', 'aggr_mlp': 1, 'fea_activation': 'ReLU'}}, 'commit_id': '313c4e48e95c178d60dfa44f2a13ef0902f41688', 'time_stamp': '1598946670', 'directory': '../../../nlg_results/dc2-Ymq2/ogbg/board/'}
Target seqence less or equal to 5 is 0.9874166466036873%.
Coverage of top 5000 vocabulary:
0.9025832389087423
Epoch 1 training...
Average training loss: 2.9738594631628232
Evaluating...
Train: 0.2361249635725295 Validation: 0.21202463467879604 Test: 0.2250357035690551 Train loss: 2.9738594631628232
Epoch 2 training...
Average training loss: 2.354800486766559
Evaluating...
Train: 0.3008640901465439 Validation: 0.26304628691007265 Test: 0.28070899773332797 Train loss: 2.354800486766559
Epoch 3 training...
Average training loss: 2.106560915708542
Evaluating...
Train: 0.33543277498172935 Validation: 0.27956366677057387 Test: 0.2997732506138739 Train loss: 2.106560915708542
Epoch 4 training...
Average training loss: 1.9100911588860277
Evaluating...
Train: 0.37144420175849735 Validation: 0.2965867011008791 Test: 0.31627491689000714 Train loss: 1.9100911588860277
Epoch 5 training...
Average training loss: 1.7378564371144307
Evaluating...
Train: 0.39966408396861 Validation: 0.3002904817494816 Test: 0.31962600836985744 Train loss: 1.7378564371144307
Epoch 6 training...
Average training loss: 1.5836706887299623
Evaluating...
Train: 0.43901941060551136 Validation: 0.3073023923771174 Test: 0.3256296070128766 Train loss: 1.5836706887299623
Epoch 7 training...
Average training loss: 1.4461643686483017
Evaluating...
Train: 0.47618633376267633 Validation: 0.3092045524902605 Test: 0.3304275243517999 Train loss: 1.4461643686483017
Epoch 8 training...
Average training loss: 1.3229982952214543
Evaluating...
Train: 0.5017171580530968 Validation: 0.3056005461865128 Test: 0.326339799357842 Train loss: 1.3229982952214543
Epoch 9 training...
Average training loss: 1.2128618112584129
Evaluating...
Train: 0.5268679307483943 Validation: 0.30327204387554124 Test: 0.3237259490403285 Train loss: 1.2128618112584129
Epoch 10 training...
Traceback (most recent call last):
  File "./main.py", line 276, in <module>
    main()
  File "./main.py", line 233, in main
    train_loss = train(model, device, train_loader, optimizer)
  File "./main.py", line 42, in train
    pred_list = model(batch)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/space/github_private/gnnzoo/ogbg/code/model.py", line 72, in forward
    nr = F.dropout(nr, p=self.dropout, training=self.training)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py", line 936, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 438.00 MiB (GPU 0; 22.38 GiB total capacity; 19.30 GiB already allocated; 381.56 MiB free; 21.40 GiB reserved in total by PyTorch)
