{'dataset_name': 'ogbg-code', 'seed': 777, 'num_workers': 0, 'feature': 'full', 'max_seq_len': 5, 'num_vocab': 5000, 'hyperparams': {'batch_size': 128, 'epochs': 31, 'learning_rate': 0.0001, 'step_size': 30, 'decay_rate': 0.85}, 'architecture': {'layers': 5, 'hidden': 256, 'pooling': 'add', 'JK': 'cat', 'nonlinear_conv': 'EB4', 'dropout': 0, 'variants': {'BN': 'N', 'aggr_mlp': 1, 'fea_activation': 'ReLU'}}, 'commit_id': '313c4e48e95c178d60dfa44f2a13ef0902f41688', 'time_stamp': '1598176611', 'directory': '../../../nlg_results/dc2-Ymq2/ogbg/board/'}
Target seqence less or equal to 5 is 0.9874166466036873%.
Coverage of top 5000 vocabulary:
0.9025832389087423
Epoch 1 training...
Average training loss: 4.120514788995573
Evaluating...
Train: 0.06809325185314215 Validation: 0.06257886878476789 Test: 0.06269946464041597 Train loss: 4.120514788995573
Epoch 2 training...
Average training loss: 3.2397777923526547
Evaluating...
Train: 0.10833727902680819 Validation: 0.0996114071803251 Test: 0.10318589511454464 Train loss: 3.2397777923526547
Epoch 3 training...
